{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is based on sklearn's official tutorial [Working With Text Data](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#bags-of-words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the 20 newsgroups dataset\n",
    "The dataset is called “Twenty Newsgroups”. Here is the official description, quoted from the [website](http://people.csail.mit.edu/jrennie/20Newsgroups/):\n",
    "\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
    "\n",
    "In the following we will use the built-in dataset loader for 20 newsgroups from scikit-learn. Alternatively, it is possible to download the dataset manually from the website and use the [sklearn.datasets.load_files](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html#sklearn.datasets.load_files) function by pointing it to the 20news-bydate-train sub-folder of the uncompressed archive folder.\n",
    "\n",
    "In order to get faster execution times for this first example we will work on a partial dataset with only 4 categories out of the 20 available in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned dataset is a scikit-learn “bunch”: a simple holder object with fields that can be both accessed as python dict keys or object attributes for convenience, for instance the target_names holds the list of the requested category names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mappings between target_names and integer label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism                   0\n",
      "comp.graphics                 1\n",
      "sci.med                       2\n",
      "soc.religion.christian        3\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(twenty_train.target_names):\n",
    "    print(f'{x:25} {i:5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let’s print the first loaded file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "-- \n",
      "Michael Collier (Programmer)                 The Computer Unit,\n",
      "Email: M.P.Collier@uk.ac.city                The City University,\n",
      "Tel: 071 477-8000 x3769                      London,\n",
      "Fax: 071 477-8565                            EC1V 0HB.\n",
      "\n",
      "comp.graphics 1\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(twenty_train['data'][0].split('\\n')))\n",
    "print(twenty_train.target_names[twenty_train.target[0]], twenty_train.target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features from text files¶\n",
    "## Bag of Words\n",
    "The most intuitive way to do so is to use a bags of words representation:\n",
    "\n",
    "* Assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices).\n",
    "\n",
    "* For each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j where j is the index of word w in the dictionary.\n",
    "\n",
    "<img src='images/bag_of_words.jpeg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing text with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing, tokenizing and filtering of stopwords are all included in [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer), which builds a dictionary of features and transforms documents to feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) supports counts of N-grams of words or consecutive characters. Once fitted, the vectorizer has built a dictionary of feature indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get('algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From occurrences to frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\n",
    "\n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called [tf–idf](https://en.wikipedia.org/wiki/Tf-idf) for “Term Frequency times Inverse Document Frequency”.\n",
    "\n",
    "Both tf and tf–idf can be computed as follows using [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer):\n",
    "\n",
    "<img src='images/TF-IDF.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a linear [support vector machine (SVM)](https://scikit-learn.org/stable/modules/svm.html#svm)\n",
    "\n",
    "<img src='images/svm.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9101198402130493\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None)),\n",
    " ])\n",
    "\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_clf.predict(twenty_test['data'])\n",
    "print(f'Test Accuracy: {np.mean(predicted == twenty_test.target)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.95      0.80      0.87       319\n",
      "         comp.graphics       0.87      0.98      0.92       389\n",
      "               sci.med       0.94      0.89      0.91       396\n",
      "soc.religion.christian       0.90      0.95      0.93       398\n",
      "\n",
      "              accuracy                           0.91      1502\n",
      "             macro avg       0.91      0.91      0.91      1502\n",
      "          weighted avg       0.91      0.91      0.91      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter tuning using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "     'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "     'tfidf__use_idf': (True, False),\n",
    "     'clf__alpha': (1e-2, 1e-3),\n",
    " }\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "<img src='images/crossValidation.png'>\n",
    "\n",
    "### Grid Search\n",
    "<img src='images/gs.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9641116526362428\n",
      "clf__alpha: 0.001\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f'Best Score: {gs_clf.best_score_}')\n",
    "for param_name in sorted(parameters.keys()):\n",
    "     print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__alpha</th>\n",
       "      <th>param_tfidf__use_idf</th>\n",
       "      <th>param_vect__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.105948</td>\n",
       "      <td>0.024078</td>\n",
       "      <td>0.250149</td>\n",
       "      <td>0.012674</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'clf__alpha': 0.01, 'tfidf__use_idf': True, '...</td>\n",
       "      <td>0.904867</td>\n",
       "      <td>0.915929</td>\n",
       "      <td>0.858407</td>\n",
       "      <td>0.887168</td>\n",
       "      <td>0.908686</td>\n",
       "      <td>0.894993</td>\n",
       "      <td>0.020614</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.668365</td>\n",
       "      <td>0.059414</td>\n",
       "      <td>0.586363</td>\n",
       "      <td>0.016104</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'clf__alpha': 0.01, 'tfidf__use_idf': True, '...</td>\n",
       "      <td>0.935841</td>\n",
       "      <td>0.926991</td>\n",
       "      <td>0.891593</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.913140</td>\n",
       "      <td>0.915817</td>\n",
       "      <td>0.015099</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.078371</td>\n",
       "      <td>0.012068</td>\n",
       "      <td>0.232094</td>\n",
       "      <td>0.010825</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'clf__alpha': 0.01, 'tfidf__use_idf': False, ...</td>\n",
       "      <td>0.820796</td>\n",
       "      <td>0.787611</td>\n",
       "      <td>0.747788</td>\n",
       "      <td>0.803097</td>\n",
       "      <td>0.781737</td>\n",
       "      <td>0.788214</td>\n",
       "      <td>0.024345</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.685271</td>\n",
       "      <td>0.114849</td>\n",
       "      <td>0.607813</td>\n",
       "      <td>0.022694</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'clf__alpha': 0.01, 'tfidf__use_idf': False, ...</td>\n",
       "      <td>0.825221</td>\n",
       "      <td>0.809735</td>\n",
       "      <td>0.774336</td>\n",
       "      <td>0.823009</td>\n",
       "      <td>0.804009</td>\n",
       "      <td>0.807266</td>\n",
       "      <td>0.018295</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.139356</td>\n",
       "      <td>0.036813</td>\n",
       "      <td>0.290371</td>\n",
       "      <td>0.010768</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'clf__alpha': 0.001, 'tfidf__use_idf': True, ...</td>\n",
       "      <td>0.964602</td>\n",
       "      <td>0.962389</td>\n",
       "      <td>0.957965</td>\n",
       "      <td>0.964602</td>\n",
       "      <td>0.971047</td>\n",
       "      <td>0.964112</td>\n",
       "      <td>0.004222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.239864</td>\n",
       "      <td>0.019556</td>\n",
       "      <td>0.575046</td>\n",
       "      <td>0.073604</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'clf__alpha': 0.001, 'tfidf__use_idf': True, ...</td>\n",
       "      <td>0.966814</td>\n",
       "      <td>0.962389</td>\n",
       "      <td>0.951327</td>\n",
       "      <td>0.955752</td>\n",
       "      <td>0.959911</td>\n",
       "      <td>0.959238</td>\n",
       "      <td>0.005342</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.284911</td>\n",
       "      <td>0.078157</td>\n",
       "      <td>0.308939</td>\n",
       "      <td>0.055807</td>\n",
       "      <td>0.001</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'clf__alpha': 0.001, 'tfidf__use_idf': False,...</td>\n",
       "      <td>0.915929</td>\n",
       "      <td>0.896018</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.904867</td>\n",
       "      <td>0.935412</td>\n",
       "      <td>0.912716</td>\n",
       "      <td>0.013153</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.018627</td>\n",
       "      <td>0.164027</td>\n",
       "      <td>0.323899</td>\n",
       "      <td>0.047419</td>\n",
       "      <td>0.001</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'clf__alpha': 0.001, 'tfidf__use_idf': False,...</td>\n",
       "      <td>0.931416</td>\n",
       "      <td>0.922566</td>\n",
       "      <td>0.918142</td>\n",
       "      <td>0.926991</td>\n",
       "      <td>0.933185</td>\n",
       "      <td>0.926451</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       1.105948      0.024078         0.250149        0.012674   \n",
       "1       3.668365      0.059414         0.586363        0.016104   \n",
       "2       1.078371      0.012068         0.232094        0.010825   \n",
       "3       3.685271      0.114849         0.607813        0.022694   \n",
       "4       1.139356      0.036813         0.290371        0.010768   \n",
       "5       4.239864      0.019556         0.575046        0.073604   \n",
       "6       1.284911      0.078157         0.308939        0.055807   \n",
       "7       3.018627      0.164027         0.323899        0.047419   \n",
       "\n",
       "  param_clf__alpha param_tfidf__use_idf param_vect__ngram_range  \\\n",
       "0             0.01                 True                  (1, 1)   \n",
       "1             0.01                 True                  (1, 2)   \n",
       "2             0.01                False                  (1, 1)   \n",
       "3             0.01                False                  (1, 2)   \n",
       "4            0.001                 True                  (1, 1)   \n",
       "5            0.001                 True                  (1, 2)   \n",
       "6            0.001                False                  (1, 1)   \n",
       "7            0.001                False                  (1, 2)   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'clf__alpha': 0.01, 'tfidf__use_idf': True, '...           0.904867   \n",
       "1  {'clf__alpha': 0.01, 'tfidf__use_idf': True, '...           0.935841   \n",
       "2  {'clf__alpha': 0.01, 'tfidf__use_idf': False, ...           0.820796   \n",
       "3  {'clf__alpha': 0.01, 'tfidf__use_idf': False, ...           0.825221   \n",
       "4  {'clf__alpha': 0.001, 'tfidf__use_idf': True, ...           0.964602   \n",
       "5  {'clf__alpha': 0.001, 'tfidf__use_idf': True, ...           0.966814   \n",
       "6  {'clf__alpha': 0.001, 'tfidf__use_idf': False,...           0.915929   \n",
       "7  {'clf__alpha': 0.001, 'tfidf__use_idf': False,...           0.931416   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.915929           0.858407           0.887168           0.908686   \n",
       "1           0.926991           0.891593           0.911504           0.913140   \n",
       "2           0.787611           0.747788           0.803097           0.781737   \n",
       "3           0.809735           0.774336           0.823009           0.804009   \n",
       "4           0.962389           0.957965           0.964602           0.971047   \n",
       "5           0.962389           0.951327           0.955752           0.959911   \n",
       "6           0.896018           0.911504           0.904867           0.935412   \n",
       "7           0.922566           0.918142           0.926991           0.933185   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.894993        0.020614                6  \n",
       "1         0.915817        0.015099                4  \n",
       "2         0.788214        0.024345                8  \n",
       "3         0.807266        0.018295                7  \n",
       "4         0.964112        0.004222                1  \n",
       "5         0.959238        0.005342                2  \n",
       "6         0.912716        0.013153                5  \n",
       "7         0.926451        0.005556                3  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(gs_clf.cv_results_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
